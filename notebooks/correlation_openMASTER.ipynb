{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMASTER - correlations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyomo.environ as pyo\n",
    "import openMASTER\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the abstract model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = openMASTER.make_model()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data upload\n",
    "\n",
    "* If you haven't created the .csv files, please:\n",
    "    * Be aware the openMASTER_Data.xlsx file has to be downloaded using git-lfs or the following link:\n",
    "        https://github.com/IIT-EnergySystemModels/openMASTER/raw/main/data/input/openMASTER_Data.xlsx?download=\n",
    "    * Run the first line of code in this cell, which will both create the .csv files and load them into the DataPortal (this whole function takes several minutes)\n",
    "\n",
    "* On the contrary, if you have already created the .csv files from the Excel file and haven't changed them, you can directly go on to the second line of code. This will save some minutes.\n",
    "\n",
    "In any case, add \"#\" in front of the line you are not running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = openMASTER.load_dataportal_from_excel('../data/input/openMASTER_Data.xlsx')\n",
    "#data = openMASTER.load_dataportal_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_mean   = '..\\data\\input\\mean.csv'\n",
    "df_mean    = pd.read_csv(csv_mean, delimiter=';', header=[0])\n",
    "X_mean     = df_mean.values\n",
    "\n",
    "\n",
    "#csv_data   = '..\\data\\input\\costs_correl.csv'\n",
    "#df_data    = pd.read_csv(csv_data, delimiter=';', header=[0])\n",
    "#X_data     = df_data.values\n",
    "#X_o        = X_data - np.mean(X_data, axis=0)\n",
    "\n",
    "#X_z        = X_o / np.std (X_o, axis=0)\n",
    "#X_z        = np.nan_to_num(X_z, nan=0.0)\n",
    "\n",
    "# Encontrar las columnas con valores NaN en X_z\n",
    "#columns_with_nan = np.isnan(X_z).any(axis=0)\n",
    "\n",
    "# Eliminar las columnas con valores NaN en X_z\n",
    "#X_z_clean = X_z[:, ~columns_with_nan]\n",
    "\n",
    "# Eliminar las mismas columnas en X_o\n",
    "#X_o_clean = X_o[:, ~columns_with_nan]\n",
    "\n",
    "#print(X_o)\n",
    "\n",
    "csv_covar  = '..\\data\\input\\covar.xlsx'\n",
    "sheet_name = 'covar'\n",
    "df_covar   = pd.read_excel(csv_covar, sheet_name=sheet_name, index_col=0)\n",
    "X_covar    = df_covar.values\n",
    "\n",
    "\n",
    "csv_data  = '..\\data\\input\\covar.xlsx'\n",
    "sheet_name = 'data'\n",
    "df_data   = pd.read_excel(csv_data, sheet_name=sheet_name, index_col=0)\n",
    "X_hist    = df_data.values\n",
    "\n",
    "\n",
    "f_set   = data['f']\n",
    "unc_set = data['sUnc']\n",
    "\n",
    "\n",
    "# Compute the mean for each column\n",
    "\n",
    "#s_bar_dict = {unc: s_bar[i-1] for i, unc in enumerate(unc_set)}\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(93)  # You can use any integer value as the seed\n",
    "\n",
    "# Calcular las varianzas de la serie histórica\n",
    "variances_hist = np.var(X_hist, axis=0)\n",
    "\n",
    "# Escalar la matriz de covarianza utilizando las varianzas de la serie histórica\n",
    "scaled_covar = X_covar * variances_hist\n",
    "\n",
    "# Generar los datos sintéticos utilizando la media y la matriz de covarianza escalada\n",
    "X = np.random.multivariate_normal(np.ravel(X_mean), scaled_covar, size=10000)\n",
    "\n",
    "\n",
    "# Ensure all values are non-negative\n",
    "#X = np.maximum(X, 0)\n",
    "\n",
    "X_o = X - np.mean(X, axis=0)\n",
    "\n",
    "\n",
    "s_bar_dict = {unc: np.ravel(X_mean)[i] for i, unc in enumerate(unc_set)}\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "#we compute the singular value decomposition\n",
    "\n",
    "#we use eigvals, eigvecs so they are not ordered in decreasing order\n",
    "#eigvals, eigvecs = np.linalg.eig(np.cov(X_o.T))\n",
    "\n",
    "#eigvecs_real = np.real(eigvecs)\n",
    "#eigvecs_imag = np.imag(eigvecs)\n",
    "\n",
    "#eigvals_real = np.real(eigvals)\n",
    "#eigvals_imag = np.imag(eigvals)\n",
    "\n",
    "cov_matrix = np.cov(X_o.T)\n",
    "\n",
    "\n",
    "u, s, vh = np.linalg.svd(cov_matrix, full_matrices=True)\n",
    "#u, s, vh = np.linalg.svd(X_covar, full_matrices=True)\n",
    "\n",
    "#pca1 = PCA()\n",
    "#pca1.fit(X_o)\n",
    "\n",
    "#u2, s2, vh2 = np.linalg.svd(np.cov(X_z_clean.T), full_matrices=True)\n",
    "#pca2 = PCA()\n",
    "#pca2.fit(X_z_clean)\n",
    "\n",
    "# Comparar los componentes principales obtenidos de ambos métodos\n",
    "#print(\"Componentes principales obtenidos por el método 1:\")\n",
    "#print(pca1.components_)\n",
    "#print(\"\\nComponentes principales obtenidos por el método 2:\")\n",
    "#print(pca2.components_)\n",
    "\n",
    "# Comprobación de similitud\n",
    "#similarity = np.allclose(pca1.components_, pca2.components_)\n",
    "#print(\"\\n¿Los componentes principales son similares?\", similarity)\n",
    "\n",
    "\n",
    "##np.linalg.svd --> Singular Value Decomposition:\n",
    "##np.cov --> estimate the covariance matrix\n",
    "##u is eigenvector matrix and s are the eigenvalues\n",
    "\n",
    "#prueba = np.dot(X_o, u)\n",
    "\n",
    "#now we compute the vectors needed to build the uncertainty region\n",
    "w_max = np.max(np.dot(X_o, u) / np.linalg.norm(u, axis=0), 0)\n",
    "#w_max = np.where((w_max > -1e-03) & (w_max < 1e-03), 0, w_max)\n",
    "w_max_dict = {f_i: w_max[i] for i, f_i in enumerate(f_set)}\n",
    "#w_max_dict = {f_i: w_max[i-1] for i, f_i in enumerate(f_set)}\n",
    "\n",
    "w_min = np.min(np.dot(X_o, u) / np.linalg.norm(u, axis=0), 0)\n",
    "#w_min = np.where((w_min > -1e-03) & (w_min < 1e-03), 0, w_min)\n",
    "w_min_dict = {f_i: w_min[i] for i, f_i in enumerate(f_set)}\n",
    "#w_min_dict = {f_i: w_min[i-1] for i, f_i in enumerate(f_set)}\n",
    "\n",
    "alpha_up = (w_max / np.linalg.norm(u, axis=0)) * u\n",
    "alpha_up_dict = {(unc, f_i): alpha_up[i, j] for i, unc in enumerate(unc_set) for j, f_i in enumerate(f_set)}\n",
    "\n",
    "alpha_do = (w_min / np.linalg.norm(u, axis=0)) * u\n",
    "alpha_do_dict = {(unc, f_i): alpha_do[i, j] for i, unc in enumerate(unc_set) for j, f_i in enumerate(f_set)}\n",
    "\n",
    "rest = ((w_max + w_min) / (2 * np.linalg.norm(u, axis=0))) * u\n",
    "rest_dict = {(unc, f_i): rest[i, j] for i, unc in enumerate(unc_set) for j, f_i in enumerate(f_set)}\n",
    "\n",
    "\n",
    "data._data[None][m.pS.name]        = s_bar_dict\n",
    "data._data[None][m.pW_max.name]    = w_max_dict\n",
    "data._data[None][m.pW_min.name]    = w_min_dict\n",
    "data._data[None][m.pAlpha_up.name] = alpha_up_dict\n",
    "data._data[None][m.pAlpha_do.name] = alpha_do_dict\n",
    "data._data[None][m.pRest.name]     = rest_dict\n",
    "\n",
    "print(alpha_up_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the instance of the abstract model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = m.create_instance(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance.write('instance.lp', io_options={'symbolic_solver_labels': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data._data[None][m.pCECapex.name]\n",
    "#instance.pS.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the model instance\n",
    "\n",
    "To solve the model instance, please select a solver within the Pyomo SolverFactory. Please note that any solver has to be previously installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = pyo.SolverFactory('gurobi')\n",
    "\n",
    "solver_results = solver.solve(instance, keepfiles=False, tee=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information on variables through the model output to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path        = \"../data/input/openMASTER_Data.xlsx\"\n",
    "output_path = \"../data/tmp/output\"\n",
    "sheetname   = \"Output\"\n",
    "\n",
    "d_vars_from_instance = openMASTER.export_model_to_csv(path, output_path, sheetname, instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading variable information from .csv to a dictionary containing all outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vars = openMASTER.import_results_from_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vQSTInTE_results = d_vars['vQSTInTE']\n",
    "vQSTInTE_sum = vQSTInTE_results.groupby(['sYear', 'sTE', 'sST']).sum()\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(output_path, \"vQSTInTE_sum.csv\")\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "vQSTInTE_sum.to_csv(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vQESNS_results = d_vars['vQESNS']\n",
    "vQESNS_sum = vQESNS_results['vQESNS'].sum()\n",
    "print(\"Sum of vQESNS:    \", vQESNS_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vEmiCO2CapTraExc_sum    = d_vars['vEmiCO2CapTraExc'].sum()\n",
    "vEmiCO2CapEleExc_sum    = d_vars['vEmiCO2CapEleExc'].sum()\n",
    "vEmiCO2CapIndTEExc_sum  = d_vars['vEmiCO2CapIndTEExc'].sum()\n",
    "vEmiCO2CapIndProExc_sum = d_vars['vEmiCO2CapIndProExc'].sum()\n",
    "vEmiCO2CapOthExc_sum    = d_vars['vEmiCO2CapOthExc'].sum()\n",
    "vEmiCO2CapRefExc_sum    = d_vars['vEmiCO2CapRefExc'].sum()\n",
    "\n",
    "print(\"Sum of vEmiCO2CapTraExc:    \", vEmiCO2CapTraExc_sum)\n",
    "print(\"Sum of vEmiCO2CapEleExc:    \", vEmiCO2CapEleExc_sum)\n",
    "print(\"Sum of vEmiCO2CapIndTEExc:  \", vEmiCO2CapIndTEExc_sum)\n",
    "print(\"Sum of vEmiCO2CapIndProExc: \", vEmiCO2CapIndProExc_sum)\n",
    "print(\"Sum of vEmiCO2CapOthExc:    \", vEmiCO2CapOthExc_sum)\n",
    "print(\"Sum of vEmiCO2CapRefExc:    \", vEmiCO2CapRefExc_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0517a97abf0a6b47b3d9f8b7b88c86ebe679f61210aefcb79cf5ebc38f36513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
